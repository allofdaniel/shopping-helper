# 자동 크롤링 워크플로우
# 매일 2회 (오전 9시, 오후 9시 한국 시간) 자동 실행

name: Auto Crawl & Update

on:
  schedule:
    # 매일 오전 9시 (KST) = UTC 0시
    - cron: '0 0 * * *'
    # 매일 오후 9시 (KST) = UTC 12시
    - cron: '0 12 * * *'
  workflow_dispatch:  # 수동 실행 가능
    inputs:
      max_videos:
        description: '매장당 크롤링할 영상 수'
        required: false
        default: '15'

permissions:
  contents: write  # git push 권한

env:
  PYTHON_VERSION: '3.11'
  MAX_VIDEOS: ${{ github.event.inputs.max_videos || '15' }}

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd crawler
          pip install -r requirements.txt

      - name: Create .env file
        run: |
          cd crawler
          cat > .env << EOF
          YOUTUBE_API_KEY=${{ secrets.YOUTUBE_API_KEY }}
          GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}
          AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION=ap-southeast-2
          S3_BUCKET=notam-korea-data
          DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}
          SLACK_WEBHOOK_URL=${{ secrets.SLACK_WEBHOOK_URL }}
          GMAIL_USER=${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASSWORD=${{ secrets.GMAIL_APP_PASSWORD }}
          NOTIFY_EMAIL=${{ secrets.NOTIFY_EMAIL }}
          EOF

      - name: Run crawling pipeline
        run: |
          cd crawler
          python -c "
          import sys
          sys.stdout.reconfigure(encoding='utf-8')

          from pipeline import ImprovedDataPipeline

          max_videos = int('${{ env.MAX_VIDEOS }}')
          print(f'=== 개선된 자동 크롤링 시작 (매장당 {max_videos}개 영상) ===')

          pipeline = ImprovedDataPipeline()

          stores = ['daiso', 'costco', 'ikea', 'oliveyoung', 'convenience']

          for i, store in enumerate(stores, 1):
              print(f'\n[{i}/{len(stores)}] {store} 크롤링...')
              try:
                  result = pipeline.run_full_pipeline(store, max_videos=max_videos)
                  print(f'  -> 추출: {result.get(\"products_extracted\", 0)}개, 매칭: {result.get(\"products_matched\", 0)}개')
              except Exception as e:
                  print(f'  {store} 크롤링 실패: {e}')
                  continue

          print('\n=== 크롤링 완료 ===')
          "

      - name: Upload to S3
        continue-on-error: true
        run: |
          cd crawler
          python -c "
          import sys
          sys.stdout.reconfigure(encoding='utf-8')

          from s3_uploader import S3Uploader

          print('=== S3 업로드 시작 ===')
          uploader = S3Uploader()

          # JSON 업로드
          result = uploader.upload_json_export()
          if result:
              print(f'JSON 업로드 완료: {result}')

          # DB 백업 업로드
          db_result = uploader.upload_database()
          if db_result:
              print(f'DB 백업 업로드 완료: {db_result}')

          print('=== S3 업로드 완료 ===')
          "

      - name: Sync data to web/public/data/
        run: |
          cd crawler
          python sync_to_github.py

      - name: Commit and push data
        run: |
          git config user.email "bot@shopping-helper.local"
          git config user.name "Shopping Helper Bot"
          git add web/public/data/

          # 변경사항이 있을 때만 커밋
          if git diff --cached --quiet; then
            echo "No data changes to commit"
          else
            TIMESTAMP=$(TZ='Asia/Seoul' date '+%Y-%m-%d %H:%M')
            git commit -m "data: Auto-sync ${TIMESTAMP}"
            git push
            echo "Data synced and pushed!"
          fi

      - name: Generate summary
        continue-on-error: true
        run: |
          cd crawler
          python -c "
          import sqlite3
          import json
          import os

          db_path = os.path.join('..', 'data', 'products.db')
          if os.path.exists(db_path):
              conn = sqlite3.connect(db_path)
              cursor = conn.cursor()

              # 전체 통계
              cursor.execute('SELECT COUNT(*) FROM products WHERE is_hidden = 0')
              total = cursor.fetchone()[0]

              # 매장별 통계
              cursor.execute('''
                  SELECT store_key, COUNT(*)
                  FROM products
                  WHERE is_hidden = 0
                  GROUP BY store_key
              ''')
              stores = dict(cursor.fetchall())

              conn.close()

              print('## 크롤링 완료 보고서')
              print(f'- 전체 상품: {total}개')
              for store, count in sorted(stores.items(), key=lambda x: -x[1]):
                  print(f'- {store}: {count}개')
          else:
              print('## 크롤링 완료')
              print(f'DB not found at {db_path}')
          " >> $GITHUB_STEP_SUMMARY

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: crawl-data-${{ github.run_number }}
          path: |
            data/products.db
          retention-days: 7
          if-no-files-found: ignore

      - name: Send Discord notification on success
        if: success()
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          if [ -n "$DISCORD_WEBHOOK_URL" ]; then
            curl -H "Content-Type: application/json" \
              -d '{
                "embeds": [{
                  "title": "Crawling Completed",
                  "description": "Daily crawling job finished successfully.",
                  "color": 5763719,
                  "fields": [
                    {"name": "Run", "value": "#${{ github.run_number }}", "inline": true},
                    {"name": "Trigger", "value": "${{ github.event_name }}", "inline": true}
                  ],
                  "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
                }]
              }' \
              $DISCORD_WEBHOOK_URL
          fi

      - name: Send email report
        if: success()
        continue-on-error: true
        run: |
          cd crawler
          python email_notifier.py

      - name: Send Discord notification on failure
        if: failure()
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          if [ -n "$DISCORD_WEBHOOK_URL" ]; then
            curl -H "Content-Type: application/json" \
              -d '{
                "embeds": [{
                  "title": "Crawling Failed",
                  "description": "Daily crawling job encountered an error.",
                  "color": 15158332,
                  "fields": [
                    {"name": "Run", "value": "#${{ github.run_number }}", "inline": true},
                    {"name": "Details", "value": "[View Logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})", "inline": true}
                  ],
                  "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
                }]
              }' \
              $DISCORD_WEBHOOK_URL
          fi
