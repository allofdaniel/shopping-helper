# 자동 크롤링 워크플로우
# 매일 2회 (오전 9시, 오후 9시 한국 시간) 자동 실행

name: Auto Crawl & Update

on:
  schedule:
    # 매일 오전 9시 (KST) = UTC 0시
    - cron: '0 0 * * *'
    # 매일 오후 9시 (KST) = UTC 12시
    - cron: '0 12 * * *'
  workflow_dispatch:  # 수동 실행 가능
    inputs:
      max_videos:
        description: '매장당 크롤링할 영상 수'
        required: false
        default: '15'

env:
  PYTHON_VERSION: '3.11'
  MAX_VIDEOS: ${{ github.event.inputs.max_videos || '15' }}

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd crawler
          pip install -r requirements.txt

      - name: Create .env file
        run: |
          cd crawler
          cat > .env << EOF
          YOUTUBE_API_KEY=${{ secrets.YOUTUBE_API_KEY }}
          GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}
          AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION=ap-southeast-2
          S3_BUCKET=notam-korea-data
          DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}
          SLACK_WEBHOOK_URL=${{ secrets.SLACK_WEBHOOK_URL }}
          EOF

      - name: Run crawling pipeline
        run: |
          cd crawler
          python -c "
          import sys
          sys.stdout.reconfigure(encoding='utf-8')

          from pipeline import DataPipeline

          max_videos = int('${{ env.MAX_VIDEOS }}')
          print(f'=== 자동 크롤링 시작 (매장당 {max_videos}개 영상) ===')

          pipeline = DataPipeline()

          stores = ['daiso', 'costco', 'ikea', 'oliveyoung', 'convenience']

          for i, store in enumerate(stores, 1):
              print(f'\n[{i}/{len(stores)}] {store} 크롤링...')
              try:
                  pipeline.run_full_pipeline(store, max_videos=max_videos)
              except Exception as e:
                  print(f'  {store} 크롤링 실패: {e}')
                  continue

          print('\n=== 크롤링 완료 ===')
          "

      - name: Upload to S3
        run: |
          cd crawler
          python -c "
          import sys
          sys.stdout.reconfigure(encoding='utf-8')

          from s3_uploader import S3Uploader

          print('=== S3 업로드 시작 ===')
          uploader = S3Uploader()

          # JSON 업로드
          result = uploader.upload_json_export()
          if result:
              print(f'JSON 업로드 완료: {result}')

          # DB 백업 업로드
          db_result = uploader.upload_database()
          if db_result:
              print(f'DB 백업 업로드 완료: {db_result}')

          print('=== S3 업로드 완료 ===')
          "

      - name: Generate summary
        continue-on-error: true
        run: |
          cd crawler
          python -c "
          import sqlite3
          import json
          import os

          db_path = 'data/shopping_helper.db'
          if os.path.exists(db_path):
              conn = sqlite3.connect(db_path)
              cursor = conn.cursor()

              # 전체 통계
              cursor.execute('SELECT COUNT(*) FROM products WHERE is_hidden = 0')
              total = cursor.fetchone()[0]

              # 매장별 통계
              cursor.execute('''
                  SELECT store_key, COUNT(*)
                  FROM products
                  WHERE is_hidden = 0
                  GROUP BY store_key
              ''')
              stores = dict(cursor.fetchall())

              conn.close()

              print('## 크롤링 완료 보고서')
              print(f'- 전체 상품: {total}개')
              print(f'- 다이소: {stores.get(\"daiso\", 0)}개')
              print(f'- 코스트코: {stores.get(\"costco\", 0)}개')
              print(f'- 이케아: {stores.get(\"ikea\", 0)}개')
          else:
              print('## 크롤링 완료')
              print('S3에 업로드 완료되었습니다.')
          " >> $GITHUB_STEP_SUMMARY

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: crawl-data-${{ github.run_number }}
          path: |
            crawler/data/shopping_helper.db
            crawler/data/products_export.json
          retention-days: 7
          if-no-files-found: ignore
