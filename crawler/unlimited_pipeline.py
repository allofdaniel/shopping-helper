# -*- coding: utf-8 -*-
"""
ê¿€í…œì¥ë°”êµ¬ë‹ˆ - ë¬´ì œí•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ (yt-dlp ê¸°ë°˜)
YouTube API ì¿¼í„° ì œí•œ ì—†ì´ ëŒ€ëŸ‰ ìˆ˜ì§‘ ê°€ëŠ¥

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ì‹¤í–‰ (ë‹¤ì´ì†Œ, ê° ì±„ë„ 20ê°œ ì˜ìƒ)
    python unlimited_pipeline.py

    # íŠ¹ì • ë§¤ì¥
    python unlimited_pipeline.py --store costco

    # ëª¨ë“  ë§¤ì¥ ìˆ˜ì§‘
    python unlimited_pipeline.py --all

    # ì„œë²„ ë°ëª¬ ëª¨ë“œ (ì£¼ê¸°ì  ì‹¤í–‰)
    python unlimited_pipeline.py --daemon --interval 3600

    # ì¹´íƒˆë¡œê·¸ë§Œ í¬ë¡¤ë§ (ë‹¤ì´ì†Œëª°)
    python unlimited_pipeline.py --catalog-only
"""
import argparse
import asyncio
import time
import json
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, **kwargs):
        return iterable

from ytdlp_crawler import YTDLPCrawler, YTDLP_AVAILABLE
from transcript_validator import TranscriptValidator
from improved_product_extractor import ImprovedProductExtractor
from improved_product_matcher import ImprovedProductMatcher
from improved_database import ImprovedDatabase
from config import STORE_CATEGORIES, CRAWL_CONFIG

# ë¡œê±°
try:
    from crawler_logger import get_logger, cleanup_old_logs
    logger = get_logger("pipeline")
except ImportError:
    # í´ë°±
    class FallbackLogger:
        def info(self, msg): print(f"[INFO] {msg}")
        def debug(self, msg): pass
        def warning(self, msg): print(f"[WARN] {msg}")
        def error(self, msg, exc_info=False): print(f"[ERROR] {msg}")
        def success(self, msg): print(f"[OK] {msg}")
        def start_crawl(self, store, t): return datetime.now()
        def end_crawl(self, store, start, stats): pass
        def print_daily_summary(self): pass
    logger = FallbackLogger()
    def cleanup_old_logs(days=7): pass

# ì¹´íƒˆë¡œê·¸ í¬ë¡¤ëŸ¬
try:
    from daiso_mall_scraper import DaisoMallScraper, PLAYWRIGHT_AVAILABLE
    CATALOG_CRAWLER_AVAILABLE = PLAYWRIGHT_AVAILABLE
except ImportError:
    CATALOG_CRAWLER_AVAILABLE = False

# ì½”ìŠ¤íŠ¸ì½” í¬ë¡¤ëŸ¬
try:
    from costco_scraper import CostcoScraper, COSTCO_SEARCH_KEYWORDS
    COSTCO_SCRAPER_AVAILABLE = True
except ImportError:
    COSTCO_SCRAPER_AVAILABLE = False

# ì˜¬ë¦¬ë¸Œì˜ í¬ë¡¤ëŸ¬
try:
    from oliveyoung_scraper import OliveyoungScraper, OLIVEYOUNG_SEARCH_KEYWORDS
    OLIVEYOUNG_SCRAPER_AVAILABLE = True
except ImportError:
    OLIVEYOUNG_SCRAPER_AVAILABLE = False

# ì¿ íŒ¡ í¬ë¡¤ëŸ¬
try:
    from coupang_scraper import CoupangScraper, COUPANG_SEARCH_KEYWORDS
    COUPANG_SCRAPER_AVAILABLE = True
except ImportError:
    COUPANG_SCRAPER_AVAILABLE = False

# íŠ¸ë ˆì´ë”ìŠ¤ í¬ë¡¤ëŸ¬
try:
    from traders_scraper import TradersScraper, TRADERS_SEARCH_KEYWORDS
    TRADERS_SCRAPER_AVAILABLE = True
except ImportError:
    TRADERS_SCRAPER_AVAILABLE = False

# ì´ì¼€ì•„ í¬ë¡¤ëŸ¬
try:
    from ikea_scraper import IkeaScraper, IKEA_SEARCH_KEYWORDS
    IKEA_SCRAPER_AVAILABLE = True
except ImportError:
    IKEA_SCRAPER_AVAILABLE = False

# í¸ì˜ì  í¬ë¡¤ëŸ¬
try:
    from convenience_scraper import CUScraper, GS25Scraper, SevenElevenScraper, Emart24Scraper
    CONVENIENCE_SCRAPER_AVAILABLE = True
except ImportError:
    CONVENIENCE_SCRAPER_AVAILABLE = False

# ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§ ì„¤ì •
CATALOG_CONFIG = {
    "daiso": {
        "enabled": True,
        "crawler_class": "DaisoMallScraper",
        "categories": [
            "ìƒí™œìš©í’ˆ", "ì£¼ë°©ìš©í’ˆ", "ìš•ì‹¤ìš©í’ˆ", "ì²­ì†Œìš©í’ˆ",
            "ìˆ˜ë‚©ì •ë¦¬", "ì¸í…Œë¦¬ì–´", "ë¬¸êµ¬íŒ¬ì‹œ", "íŒŒí‹°ìš©í’ˆ",
        ],
        "keywords": [
            "ì‹¤ë¦¬ì½˜", "ìˆ˜ì„¸ë¯¸", "ë°°ìˆ˜êµ¬", "ì •ë¦¬í•¨", "ë°€íìš©ê¸°",
            "í–‰ê±°", "í›„í¬", "ìˆ˜ë‚©", "ë°”êµ¬ë‹ˆ", "íŠ¸ë ˆì´",
        ],
        "update_interval_hours": 24,  # ì¹´íƒˆë¡œê·¸ ì—…ë°ì´íŠ¸ ì£¼ê¸°
    },
    "costco": {
        "enabled": True,
        "crawler_class": "CostcoScraper",
        "keywords": [
            "ê³¼ì", "ìŠ¤ë‚µ", "ê²¬ê³¼ë¥˜", "ì´ˆì½œë¦¿", "ì»¤í”¼",
            "ìŒë£Œ", "ìƒìˆ˜", "ì£¼ìŠ¤", "ì°¨", "ìš°ìœ ",
            "ê³ ê¸°", "ì†Œê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ë‹­ê³ ê¸°", "í•´ì‚°ë¬¼",
            "ê³¼ì¼", "ì±„ì†Œ", "ìƒëŸ¬ë“œ", "ëƒ‰ë™ì‹í’ˆ", "í”¼ì",
            "ë¼ë©´", "ì¦‰ì„ë°¥", "í†µì¡°ë¦¼", "ì†ŒìŠ¤", "ì¡°ë¯¸ë£Œ",
            "ì„¸ì œ", "í™”ì¥ì§€", "ì²­ì†Œìš©í’ˆ", "ì£¼ë°©ìš©í’ˆ", "ìƒí™œìš©í’ˆ",
        ],
        "categories": [
            "/c/SpecialPriceOffers",  # ìŠ¤í˜ì…œ í• ì¸
            "/c/BuyersPick",  # Buyer's Pick
        ],
        "update_interval_hours": 24,
    },
    "oliveyoung": {
        "enabled": True,
        "crawler_class": "OliveyoungScraper",
        "keywords": [
            "ì„ í¬ë¦¼", "í´ë Œì§•", "í† ë„ˆ", "ì—ì„¼ìŠ¤", "í¬ë¦¼",
            "ë§ˆìŠ¤í¬íŒ©", "ë¦½ìŠ¤í‹±", "íŒŒìš´ë°ì´ì…˜", "ì¿ ì…˜", "ì•„ì´ë¼ì´ë„ˆ",
            "ìƒ´í‘¸", "íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸", "ë°”ë””ë¡œì…˜", "í•¸ë“œí¬ë¦¼", "í–¥ìˆ˜",
            "ë¹„íƒ€ë¯¼", "ì˜ì–‘ì œ", "ë‹¤ì´ì–´íŠ¸", "ê±´ê°•ì‹í’ˆ", "ìœ ì‚°ê· ",
        ],
        "categories": [],
        "update_interval_hours": 24,
    },
    "coupang": {
        "enabled": True,
        "crawler_class": "CoupangScraper",
        "keywords": [
            "ìƒí™œìš©í’ˆ", "ì£¼ë°©ìš©í’ˆ", "ìš•ì‹¤ìš©í’ˆ", "ì²­ì†Œìš©í’ˆ", "ìˆ˜ë‚©ì •ë¦¬",
            "ì‹í’ˆ", "ê³¼ì", "ìŒë£Œ", "ë¼ë©´", "ì¦‰ì„ì‹í’ˆ",
            "ê±´ê°•ì‹í’ˆ", "ë¹„íƒ€ë¯¼", "ìœ ì‚°ê· ", "ë‹¤ì´ì–´íŠ¸",
            "í™”ì¥í’ˆ", "ìŠ¤í‚¨ì¼€ì–´", "ë©”ì´í¬ì—…",
        ],
        "categories": [],
        "update_interval_hours": 24,
    },
    "traders": {
        "enabled": True,
        "crawler_class": "TradersScraper",
        "keywords": [
            "ê³¼ì", "ìŠ¤ë‚µ", "ìŒë£Œ", "ìš°ìœ ", "ì»¤í”¼",
            "ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì†Œê³ ê¸°", "ë‹­ê³ ê¸°",
            "ê³¼ì¼", "ì±„ì†Œ", "ëƒ‰ë™ì‹í’ˆ", "ë¼ë©´",
            "ì„¸ì œ", "í™”ì¥ì§€", "ì²­ì†Œìš©í’ˆ", "ì£¼ë°©ìš©í’ˆ",
        ],
        "categories": [],
        "update_interval_hours": 24,
    },
    "ikea": {
        "enabled": True,
        "crawler_class": "IkeaScraper",
        "keywords": [
            "ì±…ìƒ", "ì˜ì", "ìˆ˜ë‚©ì¥", "ì„ ë°˜", "ì˜·ì¥",
            "ì†ŒíŒŒ", "í…Œì´ë¸”", "ì¡°ëª…", "ê±°ìš¸", "ëŸ¬ê·¸",
            "ì£¼ë°©ìš©í’ˆ", "ì‹ê¸°", "ìˆ˜ë‚©", "ì •ë¦¬", "ë°”êµ¬ë‹ˆ",
        ],
        "categories": [],
        "update_interval_hours": 24,
    },
    "cu": {
        "enabled": True,
        "crawler_class": "CUScraper",
        "event_types": ["1+1", "2+1"],
        "update_interval_hours": 12,  # í¸ì˜ì ì€ ë” ìì£¼ ì—…ë°ì´íŠ¸
    },
    "gs25": {
        "enabled": True,
        "crawler_class": "GS25Scraper",
        "event_types": ["1+1", "2+1"],
        "update_interval_hours": 12,
    },
    "seveneleven": {
        "enabled": True,
        "crawler_class": "SevenElevenScraper",
        "event_types": ["1+1", "2+1"],
        "update_interval_hours": 12,
    },
    "emart24": {
        "enabled": True,
        "crawler_class": "Emart24Scraper",
        "event_types": ["1+1", "2+1"],
        "update_interval_hours": 12,
    },
}


class UnlimitedPipeline:
    """ë¬´ì œí•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ (yt-dlp ê¸°ë°˜)"""

    def __init__(self, use_daiso_enricher: bool = False):
        """
        Args:
            use_daiso_enricher: ë‹¤ì´ì†Œ ê³µì‹ëª° ì—°ë™ (ëŠë¦¼, ì„ íƒì )
        """
        if not YTDLP_AVAILABLE:
            raise ImportError("yt-dlpê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install yt-dlp")

        self.db = ImprovedDatabase()

        # yt-dlp í¬ë¡¤ëŸ¬ì— DB ì—°ê²° (ì¤‘ë³µ ì²´í¬ìš©)
        self.crawler = YTDLPCrawler(db=self.db)
        self.validator = TranscriptValidator()
        self.extractor = None
        self.matcher = None
        self.use_daiso_enricher = use_daiso_enricher

        self._init_ai()

    def _init_ai(self):
        """AI ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.extractor = ImprovedProductExtractor(provider="auto")
            print(f"[OK] AI ë¶„ì„ê¸° ì¤€ë¹„ë¨ ({self.extractor.provider})")
        except Exception as e:
            print(f"[!] AI API ë¯¸ì„¤ì •: {e}")
            print("    GEMINI_API_KEY ë˜ëŠ” OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”")

        try:
            self.matcher = ImprovedProductMatcher()
            catalog = self.db.get_daiso_catalog_all()
            if catalog:
                self.matcher.set_catalog(catalog)
                print(f"[OK] ìƒí’ˆ ë§¤ì¹­ê¸° ì¤€ë¹„ë¨ (ì¹´íƒˆë¡œê·¸: {len(catalog)}ê°œ)")
            else:
                print("[!] ì¹´íƒˆë¡œê·¸ ì—†ìŒ - ë§¤ì¹­ ìŠ¤í‚µë¨")
                if CATALOG_CRAWLER_AVAILABLE:
                    print("    -> 'python unlimited_pipeline.py --catalog-only' ë¡œ ì¹´íƒˆë¡œê·¸ ìˆ˜ì§‘ í•„ìš”")
        except Exception as e:
            print(f"[!] ë§¤ì¹­ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def crawl_catalog_async(self, store_key: str = "daiso") -> Dict:
        """
        ë§¤ì¥ ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§ (ë¹„ë™ê¸°)

        Args:
            store_key: ë§¤ì¥ í‚¤

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ í†µê³„
        """
        if not CATALOG_CRAWLER_AVAILABLE:
            return {"error": "Playwrightê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"}

        if store_key not in CATALOG_CONFIG:
            return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ì¥: {store_key}"}

        config = CATALOG_CONFIG[store_key]
        if not config.get("enabled"):
            return {"error": f"{store_key} ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§ ë¹„í™œì„±í™”"}

        # ë¡œê¹… ì‹œì‘
        start_time = logger.start_crawl(store_key, "catalog")

        stats = {
            "store": store_key,
            "products_crawled": 0,
            "products_saved": 0,
            "errors": [],
        }

        if store_key == "daiso":
            from daiso_mall_scraper import DaisoMallScraper

            scraper = DaisoMallScraper(headless=True)

            try:
                all_products = []

                # í‚¤ì›Œë“œë³„ ê²€ìƒ‰
                keywords = config.get("keywords", [])
                print(f"\nê²€ìƒ‰ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")

                for keyword in keywords:
                    print(f"  ê²€ìƒ‰: '{keyword}'")
                    try:
                        products = await scraper.search_products(keyword, limit=30)
                        stats["products_crawled"] += len(products)

                        for p in products:
                            all_products.append(p)
                            print(f"    - {p.name}: {p.price}ì› (í’ˆë²ˆ: {p.product_no})")

                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{keyword}: {e}")

                    await asyncio.sleep(2)  # ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€

                # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰
                categories = config.get("categories", [])
                print(f"\nì¹´í…Œê³ ë¦¬ ê²€ìƒ‰: {len(categories)}ê°œ")

                for category in categories:
                    print(f"  ì¹´í…Œê³ ë¦¬: '{category}'")
                    try:
                        products = await scraper.search_products(category, limit=50)
                        stats["products_crawled"] += len(products)

                        for p in products:
                            all_products.append(p)

                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{category}: {e}")

                    await asyncio.sleep(2)

                # ì¤‘ë³µ ì œê±° ë° DB ì €ì¥
                seen_product_nos = set()
                for p in all_products:
                    if p.product_no not in seen_product_nos:
                        seen_product_nos.add(p.product_no)
                        product_dict = {
                            "product_no": p.product_no,
                            "name": p.name,
                            "price": p.price,
                            "image_url": p.image_url,
                            "product_url": p.product_url,
                            "category": p.category,
                        }
                        if self.db.insert_daiso_product(product_dict):
                            stats["products_saved"] += 1

                print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {stats['products_crawled']}ê°œ ìˆ˜ì§‘, "
                      f"{stats['products_saved']}ê°œ ì €ì¥ (ì¤‘ë³µ ì œì™¸)")

                # ë§¤ì²˜ ì¹´íƒˆë¡œê·¸ ìƒˆë¡œê³ ì¹¨
                catalog = self.db.get_daiso_catalog_all()
                if catalog and self.matcher:
                    self.matcher.set_catalog(catalog)
                    print(f"ë§¤ì¹­ê¸° ì¹´íƒˆë¡œê·¸ ì—…ë°ì´íŠ¸: {len(catalog)}ê°œ")

            finally:
                await scraper.close()

        elif store_key == "costco":
            if not COSTCO_SCRAPER_AVAILABLE:
                return {"error": "ì½”ìŠ¤íŠ¸ì½” ìŠ¤í¬ë˜í¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            from costco_scraper import CostcoScraper

            scraper = CostcoScraper(headless=True)

            try:
                all_products = []

                # í‚¤ì›Œë“œë³„ ê²€ìƒ‰
                keywords = config.get("keywords", [])
                print(f"\nê²€ìƒ‰ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")

                for keyword in keywords:
                    print(f"  ê²€ìƒ‰: '{keyword}'")
                    try:
                        products = await scraper.search_products(keyword, limit=20)
                        stats["products_crawled"] += len(products)

                        for p in products:
                            all_products.append(p)
                            print(f"    - {p.name}: {p.price:,}ì› (ì½”ë“œ: {p.product_code})")

                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{keyword}: {e}")

                    await asyncio.sleep(2)  # ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€

                # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰
                categories = config.get("categories", [])
                print(f"\nì¹´í…Œê³ ë¦¬ ê²€ìƒ‰: {len(categories)}ê°œ")

                for category in categories:
                    print(f"  ì¹´í…Œê³ ë¦¬: '{category}'")
                    try:
                        products = await scraper.get_category_products(category, limit=50)
                        stats["products_crawled"] += len(products)

                        for p in products:
                            all_products.append(p)

                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{category}: {e}")

                    await asyncio.sleep(2)

                # ì¤‘ë³µ ì œê±° ë° DB ì €ì¥
                seen_codes = set()
                for p in all_products:
                    if p.product_code not in seen_codes:
                        seen_codes.add(p.product_code)
                        product_dict = {
                            "product_code": p.product_code,
                            "name": p.name,
                            "price": p.price,
                            "image_url": p.image_url,
                            "product_url": p.product_url,
                            "category": p.category,
                            "unit_price": p.unit_price,
                            "rating": p.rating,
                            "review_count": p.review_count,
                        }
                        if self.db.insert_costco_product(product_dict):
                            stats["products_saved"] += 1

                print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {stats['products_crawled']}ê°œ ìˆ˜ì§‘, "
                      f"{stats['products_saved']}ê°œ ì €ì¥ (ì¤‘ë³µ ì œì™¸)")

            finally:
                await scraper.close()

        elif store_key == "oliveyoung":
            if not OLIVEYOUNG_SCRAPER_AVAILABLE:
                return {"error": "ì˜¬ë¦¬ë¸Œì˜ ìŠ¤í¬ë˜í¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            from oliveyoung_scraper import OliveyoungScraper

            scraper = OliveyoungScraper(headless=True)

            try:
                all_products = []

                # í‚¤ì›Œë“œë³„ ê²€ìƒ‰
                keywords = config.get("keywords", [])
                print(f"\nê²€ìƒ‰ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")

                for keyword in keywords:
                    print(f"  ê²€ìƒ‰: '{keyword}'")
                    try:
                        products = await scraper.search_products(keyword, limit=20)
                        stats["products_crawled"] += len(products)

                        for p in products:
                            all_products.append(p)
                            print(f"    - [{p.brand}] {p.name}: {p.price:,}ì›")

                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{keyword}: {e}")

                    await asyncio.sleep(3)  # ë´‡ íƒì§€ ë°©ì§€ë¥¼ ìœ„í•œ ë” ê¸´ ë”œë ˆì´

                # ì¤‘ë³µ ì œê±° ë° DB ì €ì¥
                seen_codes = set()
                for p in all_products:
                    if p.product_code not in seen_codes:
                        seen_codes.add(p.product_code)
                        product_dict = {
                            "product_code": p.product_code,
                            "name": p.name,
                            "brand": p.brand,
                            "price": p.price,
                            "original_price": p.original_price,
                            "image_url": p.image_url,
                            "product_url": p.product_url,
                            "rating": p.rating,
                            "review_count": p.review_count,
                            "is_best": p.is_best,
                            "is_sale": p.is_sale,
                        }
                        if self.db.insert_oliveyoung_product(product_dict):
                            stats["products_saved"] += 1

                print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {stats['products_crawled']}ê°œ ìˆ˜ì§‘, "
                      f"{stats['products_saved']}ê°œ ì €ì¥ (ì¤‘ë³µ ì œì™¸)")

            finally:
                await scraper.close()

        elif store_key == "coupang":
            if not COUPANG_SCRAPER_AVAILABLE:
                return {"error": "ì¿ íŒ¡ ìŠ¤í¬ë˜í¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            from coupang_scraper import CoupangScraper

            scraper = CoupangScraper(headless=True)

            try:
                all_products = []

                # í‚¤ì›Œë“œë³„ ê²€ìƒ‰
                keywords = config.get("keywords", [])
                print(f"\nê²€ìƒ‰ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")

                for keyword in keywords:
                    print(f"  ê²€ìƒ‰: '{keyword}'")
                    try:
                        products = await scraper.search_products(keyword, limit=20)
                        stats["products_crawled"] += len(products)

                        for p in products:
                            all_products.append(p)
                            rocket = "ğŸš€" if p.is_rocket else ""
                            print(f"    - {p.name}: {p.price:,}ì› {rocket}")

                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{keyword}: {e}")

                    await asyncio.sleep(5)  # ì¿ íŒ¡ì€ ë´‡ íƒì§€ê°€ ê°•í•´ì„œ ë” ê¸´ ë”œë ˆì´

                # ì¤‘ë³µ ì œê±° ë° DB ì €ì¥
                seen_ids = set()
                for p in all_products:
                    if p.product_id not in seen_ids:
                        seen_ids.add(p.product_id)
                        product_dict = {
                            "product_id": p.product_id,
                            "name": p.name,
                            "price": p.price,
                            "original_price": p.original_price,
                            "image_url": p.image_url,
                            "product_url": p.product_url,
                            "rating": p.rating,
                            "review_count": p.review_count,
                            "is_rocket": p.is_rocket,
                            "is_rocket_fresh": p.is_rocket_fresh,
                            "seller": p.seller,
                        }
                        if self.db.insert_coupang_product(product_dict):
                            stats["products_saved"] += 1

                print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {stats['products_crawled']}ê°œ ìˆ˜ì§‘, "
                      f"{stats['products_saved']}ê°œ ì €ì¥ (ì¤‘ë³µ ì œì™¸)")

            finally:
                await scraper.close()

        elif store_key == "traders":
            if not TRADERS_SCRAPER_AVAILABLE:
                return {"error": "íŠ¸ë ˆì´ë”ìŠ¤ ìŠ¤í¬ë˜í¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            from traders_scraper import TradersScraper
            scraper = TradersScraper(headless=True)

            try:
                all_products = []
                keywords = config.get("keywords", [])
                print(f"\nê²€ìƒ‰ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")

                for keyword in keywords:
                    print(f"  ê²€ìƒ‰: '{keyword}'")
                    try:
                        products = await scraper.search_products(keyword, limit=20)
                        stats["products_crawled"] += len(products)
                        for p in products:
                            all_products.append(p)
                            print(f"    - {p.name}: {p.price:,}ì›")
                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{keyword}: {e}")
                    await asyncio.sleep(2)

                seen_ids = set()
                for p in all_products:
                    if p.item_id not in seen_ids:
                        seen_ids.add(p.item_id)
                        if self.db.insert_traders_product(p.to_dict()):
                            stats["products_saved"] += 1

                print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {stats['products_crawled']}ê°œ ìˆ˜ì§‘, "
                      f"{stats['products_saved']}ê°œ ì €ì¥")
            finally:
                await scraper.close()

        elif store_key == "ikea":
            if not IKEA_SCRAPER_AVAILABLE:
                return {"error": "ì´ì¼€ì•„ ìŠ¤í¬ë˜í¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            from ikea_scraper import IkeaScraper
            scraper = IkeaScraper(headless=True)

            try:
                all_products = []
                keywords = config.get("keywords", [])
                print(f"\nê²€ìƒ‰ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")

                for keyword in keywords:
                    print(f"  ê²€ìƒ‰: '{keyword}'")
                    try:
                        products = await scraper.search_products(keyword, limit=20)
                        stats["products_crawled"] += len(products)
                        for p in products:
                            all_products.append(p)
                            print(f"    - {p.name} ({p.type_name}): {p.price:,}ì›")
                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{keyword}: {e}")
                    await asyncio.sleep(3)

                seen_ids = set()
                for p in all_products:
                    if p.product_id not in seen_ids:
                        seen_ids.add(p.product_id)
                        if self.db.insert_ikea_product(p.to_dict()):
                            stats["products_saved"] += 1

                print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {stats['products_crawled']}ê°œ ìˆ˜ì§‘, "
                      f"{stats['products_saved']}ê°œ ì €ì¥")
            finally:
                await scraper.close()

        elif store_key in ["cu", "gs25", "seveneleven", "emart24"]:
            if not CONVENIENCE_SCRAPER_AVAILABLE:
                return {"error": "í¸ì˜ì  ìŠ¤í¬ë˜í¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            scraper_map = {
                "cu": CUScraper,
                "gs25": GS25Scraper,
                "seveneleven": SevenElevenScraper,
                "emart24": Emart24Scraper,
            }

            ScraperClass = scraper_map[store_key]
            scraper = ScraperClass(headless=True)

            try:
                all_products = []
                event_types = config.get("event_types", ["1+1"])

                for event_type in event_types:
                    print(f"  í–‰ì‚¬: '{event_type}'")
                    try:
                        products = await scraper.get_event_products(event_type, limit=30)
                        stats["products_crawled"] += len(products)
                        for p in products:
                            all_products.append(p)
                            print(f"    - {p.name}: {p.price:,}ì› [{event_type}]")
                    except Exception as e:
                        print(f"    [ì—ëŸ¬] {e}")
                        stats["errors"].append(f"{event_type}: {e}")
                    await asyncio.sleep(2)

                for p in all_products:
                    if self.db.insert_convenience_product(p.to_dict()):
                        stats["products_saved"] += 1

                logger.success(f"í¬ë¡¤ë§ ì™„ë£Œ: {stats['products_crawled']}ê°œ ìˆ˜ì§‘, "
                      f"{stats['products_saved']}ê°œ ì €ì¥")
            finally:
                await scraper.close()

        # ë¡œê¹… ì¢…ë£Œ
        logger.end_crawl(store_key, start_time, stats)
        return stats

    def crawl_catalog(self, store_key: str = "daiso") -> Dict:
        """ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§ (ë™ê¸° wrapper)"""
        return asyncio.run(self.crawl_catalog_async(store_key))

    def run(self, store_key: str = "daiso", max_videos: int = 50,
            max_per_channel: int = 20, max_per_search: int = 10,
            skip_existing: bool = True) -> Dict:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            store_key: ë§¤ì¥ í‚¤
            max_videos: ì²˜ë¦¬í•  ìµœëŒ€ ì˜ìƒ ìˆ˜
            max_per_channel: ì±„ë„ë‹¹ ìµœëŒ€ ì˜ìƒ ìˆ˜
            max_per_search: ê²€ìƒ‰ì–´ë‹¹ ìµœëŒ€ ì˜ìƒ ìˆ˜
            skip_existing: ì´ë¯¸ ì²˜ë¦¬ëœ ì˜ìƒ ìŠ¤í‚µ

        Returns:
            ì‹¤í–‰ í†µê³„
        """
        store = STORE_CATEGORIES.get(store_key)
        if not store:
            print(f"[!] ì•Œ ìˆ˜ ì—†ëŠ” ë§¤ì¥: {store_key}")
            return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” ë§¤ì¥: {store_key}"}

        start_time = datetime.now()
        print(f"\n{'='*60}")
        print(f"[ë¬´ì œí•œ íŒŒì´í”„ë¼ì¸] {store['name']} í¬ë¡¤ë§ ì‹œì‘")
        print(f"ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ì„¤ì •: ì±„ë„ë‹¹ {max_per_channel}ê°œ, ê²€ìƒ‰ì–´ë‹¹ {max_per_search}ê°œ")
        print(f"{'='*60}")

        stats = {
            "store": store_key,
            "videos_collected": 0,
            "videos_new": 0,
            "videos_with_transcript": 0,
            "products_extracted": 0,
            "products_matched": 0,
            "products_saved": 0,
            "errors": [],
        }

        # Step 1: ì˜ìƒ ìˆ˜ì§‘ (yt-dlp)
        print("\n[Step 1/5] ì˜ìƒ ìˆ˜ì§‘ (yt-dlp - ë¬´ì œí•œ)...")
        try:
            videos = self.crawler.full_crawl(
                store_key,
                max_channel_videos=max_per_channel,
                max_search_videos=max_per_search,
            )
            stats["videos_collected"] = len(videos)
            print(f"  -> {len(videos)}ê°œ ì˜ìƒ ìˆ˜ì§‘ë¨")

            # DBì— ì €ì¥ ë° ì¤‘ë³µ ì²´í¬
            new_videos = []
            for video in videos:
                existing = self.db.get_video_by_id(video['video_id'])
                if existing and skip_existing:
                    continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ì˜ìƒ ìŠ¤í‚µ
                if self.db.insert_video(video):
                    new_videos.append(video)

            stats["videos_new"] = len(new_videos)
            print(f"  -> {len(new_videos)}ê°œ ì‹ ê·œ ì˜ìƒ")

            # ì²˜ë¦¬í•  ì˜ìƒ ì œí•œ
            videos_to_process = new_videos[:max_videos]

        except Exception as e:
            print(f"  [ì—ëŸ¬] ì˜ìƒ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            stats["errors"].append(f"ì˜ìƒ ìˆ˜ì§‘: {e}")
            videos_to_process = []

        if not videos_to_process:
            print("\n[ì™„ë£Œ] ì²˜ë¦¬í•  ìƒˆ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return stats

        # Step 2: ìë§‰ ì¶”ì¶œ ë° ê²€ì¦
        print(f"\n[Step 2/5] ìë§‰ ì¶”ì¶œ ({len(videos_to_process)}ê°œ ì˜ìƒ)...")
        valid_videos = []

        for video in tqdm(videos_to_process, desc="  ìë§‰"):
            video_id = video['video_id']

            # ìë§‰ ì¶”ì¶œ
            transcript = self.crawler.get_video_transcript(video_id)

            # ìë§‰ ì—†ìœ¼ë©´ ì œëª©+ì„¤ëª… ì‚¬ìš©
            if not transcript:
                title = video.get('title', '')
                description = video.get('description', '')
                if title or description:
                    transcript = f"{title}\n\n{description}"

            # í’ˆì§ˆ ê²€ì¦
            if transcript:
                validation = self.validator.validate(transcript, store['name'])

                if validation.is_valid:
                    self.db.update_video_transcript(
                        video_id, transcript, validation.quality_score
                    )
                    valid_videos.append({
                        **video,
                        'transcript': transcript,
                        'quality_score': validation.quality_score,
                    })
                else:
                    self.db.update_video_status(video_id, 'low_quality')
            else:
                self.db.update_video_status(video_id, 'no_transcript')

            time.sleep(0.5)

        stats["videos_with_transcript"] = len(valid_videos)
        print(f"  -> ìœ íš¨í•œ ìë§‰: {len(valid_videos)}ê°œ")

        if not valid_videos:
            print("\n[ì™„ë£Œ] ìœ íš¨í•œ ìë§‰ì´ ìˆëŠ” ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return stats

        # Step 3: AI ìƒí’ˆ ì¶”ì¶œ
        if not self.extractor:
            print("\n[Step 3/5] AI ìƒí’ˆ ì¶”ì¶œ... (ìŠ¤í‚µ - AI ë¯¸ì„¤ì •)")
            all_products = []
        else:
            print(f"\n[Step 3/5] AI ìƒí’ˆ ì¶”ì¶œ ({len(valid_videos)}ê°œ ì˜ìƒ)...")
            all_products = []

            for video in tqdm(valid_videos, desc="  AI"):
                try:
                    products = self.extractor.extract_products(
                        video['transcript'], store['name']
                    )

                    for product in products:
                        product['video_id'] = video['video_id']
                        product['store_key'] = store_key
                        product['store_name'] = store['name']
                        product['source_view_count'] = video.get('view_count', 0)
                        product['channel_title'] = video.get('channel_title', '')
                        product['video_title'] = video.get('title', '')
                        all_products.append(product)

                    self.db.update_video_status(video['video_id'], 'analyzed')

                except Exception as e:
                    print(f"  [ì—ëŸ¬] {video['video_id']}: {e}")
                    stats["errors"].append(f"AI ì¶”ì¶œ: {e}")

                time.sleep(1)  # AI API ì†ë„ ì œí•œ

            stats["products_extracted"] = len(all_products)
            print(f"  -> {len(all_products)}ê°œ ìƒí’ˆ ì¶”ì¶œë¨")

        # Step 4: ë§¤ì¥ ìƒí’ˆ ë§¤ì¹­
        print("\n[Step 4/5] ë§¤ì¥ ìƒí’ˆ ë§¤ì¹­...")
        enriched_products = []

        if self.matcher and self.matcher.catalog:
            for product in tqdm(all_products, desc="  ë§¤ì¹­"):
                match = self.matcher.match(
                    product_name=product.get('name', ''),
                    price=product.get('price'),
                    category=product.get('category'),
                    keywords=product.get('keywords', []),
                )

                if match:
                    product['official'] = match.to_dict()
                    product['needs_manual_review'] = match.needs_manual_review
                    stats["products_matched"] += 1
                else:
                    product['official'] = {}
                    product['needs_manual_review'] = True

                enriched_products.append(product)

            print(f"  -> {stats['products_matched']}/{len(enriched_products)}ê°œ ë§¤ì¹­")
        else:
            enriched_products = all_products
            for p in enriched_products:
                p['needs_manual_review'] = True
            print("  -> ë§¤ì¹­ ìŠ¤í‚µ (ì¹´íƒˆë¡œê·¸ ì—†ìŒ)")

        # Step 5: DB ì €ì¥
        print("\n[Step 5/5] DB ì €ì¥...")
        for product in enriched_products:
            if self.db.insert_product(product):
                stats["products_saved"] += 1

        print(f"  -> {stats['products_saved']}ê°œ ì €ì¥ë¨")

        # ê²°ê³¼ ìš”ì•½
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()
        stats["elapsed_seconds"] = elapsed

        print(f"\n{'='*60}")
        print(f"[ë¬´ì œí•œ íŒŒì´í”„ë¼ì¸] ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"ìˆ˜ì§‘ ì˜ìƒ: {stats['videos_collected']}")
        print(f"ì‹ ê·œ ì˜ìƒ: {stats['videos_new']}")
        print(f"ìœ íš¨ ìë§‰: {stats['videos_with_transcript']}")
        print(f"ì¶”ì¶œ ìƒí’ˆ: {stats['products_extracted']}")
        print(f"ë§¤ì¹­ ì„±ê³µ: {stats['products_matched']}")
        print(f"ì €ì¥ ì™„ë£Œ: {stats['products_saved']}")

        if stats["errors"]:
            print(f"\nì—ëŸ¬: {len(stats['errors'])}ê±´")

        return stats

    def run_all_stores(self, **kwargs) -> Dict[str, Dict]:
        """ëª¨ë“  ë§¤ì¥ ìˆ˜ì§‘"""
        results = {}

        stores = list(STORE_CATEGORIES.keys())
        print(f"\n=== ì „ì²´ ë§¤ì¥ ìˆ˜ì§‘ ì‹œì‘ ({len(stores)}ê°œ ë§¤ì¥) ===")

        for i, store_key in enumerate(stores, 1):
            print(f"\n[{i}/{len(stores)}] {store_key}")
            try:
                results[store_key] = self.run(store_key, **kwargs)
            except Exception as e:
                print(f"  [ì—ëŸ¬] {store_key} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                results[store_key] = {"error": str(e)}

        # ì „ì²´ í†µê³„
        total_videos = sum(r.get('videos_collected', 0) for r in results.values())
        total_products = sum(r.get('products_saved', 0) for r in results.values())

        print(f"\n=== ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ ===")
        print(f"ì´ ì˜ìƒ: {total_videos}ê°œ")
        print(f"ì´ ìƒí’ˆ: {total_products}ê°œ")

        return results

    def run_daemon(self, interval_seconds: int = 3600, stores: List[str] = None,
                   catalog_interval_hours: int = 24):
        """
        ë°ëª¬ ëª¨ë“œ - ì£¼ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘ ì‹¤í–‰

        Args:
            interval_seconds: ì˜ìƒ ìˆ˜ì§‘ ê°„ê²© (ì´ˆ)
            stores: ìˆ˜ì§‘í•  ë§¤ì¥ ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            catalog_interval_hours: ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§ ê°„ê²© (ì‹œê°„)
        """
        if stores is None:
            stores = list(STORE_CATEGORIES.keys())

        logger.info("=== ë°ëª¬ ëª¨ë“œ ì‹œì‘ ===")
        logger.info(f"ìˆ˜ì§‘ ëŒ€ìƒ: {', '.join(stores)}")
        logger.info(f"ì˜ìƒ ìˆ˜ì§‘ ê°„ê²©: {interval_seconds}ì´ˆ ({interval_seconds/3600:.1f}ì‹œê°„)")
        logger.info(f"ì¹´íƒˆë¡œê·¸ ì—…ë°ì´íŠ¸ ê°„ê²©: {catalog_interval_hours}ì‹œê°„")
        logger.info("ì¢…ë£Œ: Ctrl+C")

        # ì‹œì‘ ì‹œ ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬
        cleanup_old_logs(days=7)

        run_count = 0
        last_catalog_crawl = None
        last_day = datetime.now().date()

        while True:
            run_count += 1
            logger.info(f"{'='*60}")
            logger.info(f"[ì‹¤í–‰ #{run_count}] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"{'='*60}")

            # ì¼ì¼ ë¦¬ì…‹ ì‹œ í†µê³„ ì¶œë ¥ ë° ë¡œê·¸ ì •ë¦¬
            current_day = datetime.now().date()
            if current_day != last_day:
                logger.print_daily_summary()
                cleanup_old_logs(days=7)
                last_day = current_day

            # ì¹´íƒˆë¡œê·¸ ì—…ë°ì´íŠ¸ ì²´í¬ (ì£¼ê¸°ì ìœ¼ë¡œ)
            if CATALOG_CRAWLER_AVAILABLE:
                should_crawl_catalog = False
                if last_catalog_crawl is None:
                    should_crawl_catalog = True
                elif datetime.now() - last_catalog_crawl > timedelta(hours=catalog_interval_hours):
                    should_crawl_catalog = True

                if should_crawl_catalog:
                    logger.info("[ì¹´íƒˆë¡œê·¸ ì—…ë°ì´íŠ¸ ì‹œì‘]")
                    for store_key in stores:
                        if store_key in CATALOG_CONFIG:
                            try:
                                self.crawl_catalog(store_key)
                            except Exception as e:
                                logger.error(f"ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({store_key}): {e}")
                    last_catalog_crawl = datetime.now()

            # ì˜ìƒ ìˆ˜ì§‘
            for store_key in stores:
                try:
                    self.run(store_key, max_videos=20)
                except Exception as e:
                    logger.error(f"{store_key}: {e}")

            logger.info(f"ë‹¤ìŒ ì‹¤í–‰ê¹Œì§€ {interval_seconds}ì´ˆ ëŒ€ê¸°...")
            time.sleep(interval_seconds)


def main():
    parser = argparse.ArgumentParser(description="ê¿€í…œì¥ë°”êµ¬ë‹ˆ ë¬´ì œí•œ íŒŒì´í”„ë¼ì¸")
    parser.add_argument('--store', type=str, default='daiso', help='ìˆ˜ì§‘í•  ë§¤ì¥ (ê¸°ë³¸: daiso)')
    parser.add_argument('--all', action='store_true', help='ëª¨ë“  ë§¤ì¥ ìˆ˜ì§‘')
    parser.add_argument('--max-videos', type=int, default=50, help='ì²˜ë¦¬í•  ìµœëŒ€ ì˜ìƒ ìˆ˜')
    parser.add_argument('--max-per-channel', type=int, default=20, help='ì±„ë„ë‹¹ ìµœëŒ€ ì˜ìƒ ìˆ˜')
    parser.add_argument('--daemon', action='store_true', help='ë°ëª¬ ëª¨ë“œ (ì£¼ê¸°ì  ì‹¤í–‰)')
    parser.add_argument('--interval', type=int, default=3600, help='ë°ëª¬ ì‹¤í–‰ ê°„ê²© (ì´ˆ)')
    parser.add_argument('--catalog-only', action='store_true', help='ì¹´íƒˆë¡œê·¸ë§Œ í¬ë¡¤ë§')
    parser.add_argument('--with-catalog', action='store_true', help='ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§ í›„ ì˜ìƒ ìˆ˜ì§‘')

    args = parser.parse_args()

    pipeline = UnlimitedPipeline()

    # ì¹´íƒˆë¡œê·¸ë§Œ í¬ë¡¤ë§
    if args.catalog_only:
        if not CATALOG_CRAWLER_AVAILABLE:
            print("[ì—ëŸ¬] Playwrightê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
            print("       pip install playwright && playwright install chromium")
            sys.exit(1)
        pipeline.crawl_catalog(args.store)
        return

    # ì¹´íƒˆë¡œê·¸ ë¨¼ì € í¬ë¡¤ë§ í›„ ì˜ìƒ ìˆ˜ì§‘
    if args.with_catalog and CATALOG_CRAWLER_AVAILABLE:
        print("\n[Step 0] ì¹´íƒˆë¡œê·¸ í¬ë¡¤ë§...")
        pipeline.crawl_catalog(args.store)

    if args.daemon:
        stores = None if args.all else [args.store]
        pipeline.run_daemon(interval_seconds=args.interval, stores=stores)
    elif args.all:
        pipeline.run_all_stores(
            max_videos=args.max_videos,
            max_per_channel=args.max_per_channel,
        )
    else:
        pipeline.run(
            store_key=args.store,
            max_videos=args.max_videos,
            max_per_channel=args.max_per_channel,
        )


if __name__ == "__main__":
    main()
